Steps

Optional
The Informatica EDC template generates internal hostnames that are not reachable. You can change your /etc/hosts (check the location if you're running on Windows) to add the VMs generated by the template:
<public-IP> infaserver <DNS name as listed in the Azure Portal>
<public-IP> infadbv <DNS name as listed in the Azure Portal>
<public-IP> infaihs0 <DNS name as listed in the Azure Portal>

e.g.
13.80.115.190 infaserver infaserveril26zi7vnj4uo.westeurope.cloudapp.azure.com
137.117.158.169 infadbv jacmetademo-edc.westeurope.cloudapp.azure.com
137.117.148.65 infaihs0 infaihsil26zi7vnj4uo.westeurope.cloudapp.azure.com


Azure Portal
create a storage account in a new resource group demoEG_Storage
Account name: jacdemometadatastorage
Account kind: StorageV2
Replication: Locally-redundant storage (LRS)
Blob access tier (default): Cool
Hierarchical namespace: Enabled

Create a container named demo-container
Generate a SAS and connect string

Informatica Administrator Console
1. Create a connection for the blob storage. Name it demoBlobStorage. Use an Account Key (easy) or SAS token (tricky).
   container name: demo-container

EDC Admin
1. Create a resource for the blob storage
More info: 
https://kb.informatica.com/howto/6/Pages/23/578848.aspx
Blob Endpoint URL: https://jacdemometadatastorage.blob.core.windows.net/demo-container

In Data Discovery specify demoBlobStorage as Source Connection Name.
Disable Business Term Accociation


====

Database setup in SQLServer
1. create demoSource database and tblMetaDemo table. Insert some data.
2. create demoTarget database and tblMetaDemo table

Informatica Administrator Tool
1. Create connections to the above created databases

Informatica Catalog Admin
1. Create a new Data Domain Group named DemoDataDomainGroup
2. Create a new Data Domain DemoDataDomainTypes, specify .*[Dd]emo.* as Data Rule RegEx and associate it with DemoDataDomainGroup
3. Create and run two Resources
	demoSource
		Basic Profile Settings
			Profile Run Option: Column Profile and Data Domain Discovery
			Domain Discovery Type: Run Discovery on Source Data
			Select Data Domain DemoDataDomainGroup
			Exclude Nulls
	demoTarget - do not run Data Domain Discovery


=========
Excellent example on custom models and lineage:
https://network.informatica.com/docs/DOC-18662
When importing the custom model, an error may appear "Content not allowed in prolog". This error occurred, for me, using Safari as well as Chrome on MacOS. Using Chrome on Windows worked fine.


Info
====
To get the object UID use:
https://<EDCHost>:<Port>/ldmcatalog/main/ldmObjectView/(%27$obj%27:%27oracle_nbabuc:___ORCL/NBABUC/TEST%27,%27$type%27:com.infa.ldm.relational.Table,%27$where%27:ldm.ThreeSixtyView)

Construct:
https://<EDCHost>:<port>/ldmcatalog/main/ldmObjectView/('$obj':'<ResourceName>:___<databasename>/<schemaname>/<tablename>','$type':com.infa.ldm.relational.Table,'$where':ldm.ThreeSixtyView)
e.g.
https://infaserveril26zi7vnj4uo.westeurope.cloudapp.azure.com:9085/ldmcatalog/main/ldmObjectView/('$obj':'demoSource:___demosource/dbo/tblMetaDemo','$type':com.infa.ldm.relational.Table,'$where':ldm.ThreeSixtyView)
Then export the asset and check the csv for the id. In the example, the id is demoSource://demosource/dbo/tblMetaDemo. Format: ResourceName://databasename/schemaname/tablename


Python
create a new virtual env underneath the python folder:
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

(follow the steps to create a .env)

PyCharm
1. Open the python folder in PyCharm to create a new project
2. In the run configuration select the above created virtual env

To test dbSchemaReplicationLineage.py:
Configure the run configuration with the following parameters:
--leftschema dbo --rightschema dbo --leftresource demoSource --rightresource demoTarget

This will generate a schemaLineage_dbo_dbo.csv file in the out folder with the following content:

Association,From Connection,To Connection,From Object,To Object
core.DataSourceDataFlow,,,demoSource://demosource/dbo,demoTarget://demotarget/dbo
core.DataSetDataFlow,,,demoSource://demosource/dbo/tblMetaDemo,demoTarget://demotarget/dbo/tblMetaDemo
core.DirectionalDataFlow,,,demoSource://demosource/dbo/tblMetaDemo/ActiveFrom,demoTarget://demotarget/dbo/tblMetaDemo/ActiveFrom
core.DirectionalDataFlow,,,demoSource://demosource/dbo/tblMetaDemo/Id,demoTarget://demotarget/dbo/tblMetaDemo/Id
core.DirectionalDataFlow,,,demoSource://demosource/dbo/tblMetaDemo/ActiveUntil,demoTarget://demotarget/dbo/tblMetaDemo/ActiveUntil
core.DirectionalDataFlow,,,demoSource://demosource/dbo/tblMetaDemo/Type,demoTarget://demotarget/dbo/tblMetaDemo/Type
core.DirectionalDataFlow,,,demoSource://demosource/dbo/tblMetaDemo/Version,demoTarget://demotarget/dbo/tblMetaDemo/Version
core.DirectionalDataFlow,,,demoSource://demosource/dbo/tblMetaDemo/Description,demoTarget://demotarget/dbo/tblMetaDemo/Description
core.DirectionalDataFlow,,,demoSource://demosource/dbo/tblMetaDemo/UUID,demoTarget://demotarget/dbo/tblMetaDemo/UUID
core.DirectionalDataFlow,,,demoSource://demosource/dbo/tblMetaDemo/Status,demoTarget://demotarget/dbo/tblMetaDemo/Status

For more info on the csv (including a flatfile example), check: https://kb.informatica.com/howto/6/Pages/20/522394.aspx


Once you have the csv it is easy to run API calls to add the lineage info.
Instead of generating the csv, it is of course also possible to change the python code to directly call the EDC APIs instead of generating the csv. To limit the complexity of this demo, and given the fact the Informatica already had created the code to generate the csv, in this demo we read the csv and call the API for each line.

More on providing lineage info on column level: https://kb.informatica.com/howto/6/Pages/22/565432.aspx
More on providing lineage info on dataset level: https://kb.informatica.com/howto/6/Pages/22/565430.aspx

Note: You can directly map columns, i.e. you do not have to map data sets first. However, you also want a lineage visual on data set level. So also do that

======
Try it using swagger to map the source database to the target database
======

https://infaserveril26zi7vnj4uo.westeurope.cloudapp.azure.com:9085/access/swagger-ui.html#
In "Select a spec" choose "internal"
In data-controller-v-1 choose Update Objects
Click "Try it out" and enter the following in the eobjs section:

{ "providerId": "enrichment", 
"modifiedBy": "Administrator", 
"updates": [ 
 { "newSourceLinks": 
 [  { "objectId": "demoSource://demosource",
  "properties": 
  [ 
   { "attrUuid": "core.targetAttribute", 
   "value": "core.DataSetDataFlow" 
   } 
  ], 
  "associationId": "core.DataSetDataFlow" 
  } 
 ], 
 "deleteFacts": [ ], 
 "newFacts": [ ], 
 "deleteSourceLinks": [ ], 
 "id": "demoTarget://demotarget" 
 } 
] }


======
Try it using swagger to map the source schema to the target schema
======

https://infaserveril26zi7vnj4uo.westeurope.cloudapp.azure.com:9085/access/swagger-ui.html#
In "Select a spec" choose "internal"
In data-controller-v-1 choose Update Objects
Click "Try it out" and enter the following in the eobjs section:

{ "providerId": "enrichment", 
"modifiedBy": "Administrator", 
"updates": [ 
 { "newSourceLinks": 
 [  { "objectId": "demoSource://demosource/dbo",
  "properties": 
  [ 
   { "attrUuid": "core.targetAttribute", 
   "value": "core.DataSetDataFlow" 
   } 
  ], 
  "associationId": "core.DataSetDataFlow" 
  } 
 ], 
 "deleteFacts": [ ], 
 "newFacts": [ ], 
 "deleteSourceLinks": [ ], 
 "id": "demoTarget://demotarget/dbo" 
 } 
] }



======
Try it using swagger to map the data set
======
More on providing lineage info on dataset level: https://kb.informatica.com/howto/6/Pages/22/565430.aspx

https://infaserveril26zi7vnj4uo.westeurope.cloudapp.azure.com:9085/access/swagger-ui.html#
In "Select a spec" choose "internal"
In data-controller-v-1 choose Update Objects
Click "Try it out" and enter the following in the eobjs section:

{ "providerId": "enrichment", 
"modifiedBy": "Administrator", 
"updates": [ 
 { "newSourceLinks": 
 [  { "objectId": "demoSource://demosource/dbo/tblMetaDemo",
  "properties": 
  [ 
   { "attrUuid": "core.targetAttribute", 
   "value": "core.DataSetDataFlow" 
   } 
  ], 
  "associationId": "core.DataSetDataFlow" 
  } 
 ], 
 "deleteFacts": [ ], 
 "newFacts": [ ], 
 "deleteSourceLinks": [ ], 
 "id": "demoTarget://demotarget/dbo/tblMetaDemo" 
 } 
] }

Click 'Execute'
Go to the catalog and check the lineage visual now shows tblMetaDemo from source to target

======
Try it using swagger to map one of the columns
======
https://infaserveril26zi7vnj4uo.westeurope.cloudapp.azure.com:9085/access/swagger-ui.html#
In "Select a spec" choose "internal"
In data-controller-v-1 choose Update Objects
Click "Try it out" and enter the following in the eobjs section:

{"providerId": "enrichment", 
"modifiedBy": "Administrator", 
"updates": [ 
 { "newSourceLinks": 
 [  { "objectId": "demoSource://demosource/dbo/tblMetaDemo/ActiveFrom",
  "properties": 
  [ 
   { "attrUuid": "core.targetAttribute", 
   "value": "core.DirectionalDataFlow" 
   } 
  ], 
  "associationId": "core.DirectionalDataFlow" 
  } 
 ], 
 "deleteFacts": [ ], 
 "newFacts": [ ], 
 "deleteSourceLinks": [ ], 
 "id": "demoTarget://demotarget/dbo/tblMetaDemo/ActiveFrom" 
 } 
] 
}

Click 'Execute'
Go to the catalog and check the lineage visual now shows the column lineage from source to target


====
CODE
====
To add the metadata-registry-interface-specifications git repository in the python repo:
https://stackoverflow.com/questions/1811730/how-do-i-work-with-a-git-repository-within-another-repository

For both source and target:
1. Check if resource exists, if not create it
   https://infaserveril26zi7vnj4uo.westeurope.cloudapp.azure.com:9085/access/1/catalog/resources
also this? https://infaserveril26zi7vnj4uo.westeurope.cloudapp.azure.com:9085/access/1/catalog/resources/demoSource/dynamicoptionvalues

2. Check if database exists (not if resource is Oracle), if not create it
3. Check if schema exists, if not create it
4. Check if table exists, if not create it (including columns)



=========
FROM
https://network.informatica.com/thread/91262

you can use the objects endpoint (2/catalog/data/objects) - passing a q (query) syntax to list all DataSet objects.
 
the query syntax could be:-
to get all datasets (dataset is a superclass - so any child class like com.infa.ldm.relational.Table com.infa.ldm.relational.View and others would be returned
core.allclassTypes:core.DataSet
or to filter for datasets within a specific resource name
+core.allclassTypes:core.DataSet +core.resourceName:"acme_crm"
or you could also filter on the resource type using core.resourceType
+core.allclassTypes:core.DataSet +core.resourceType:Oracle
from there - you just need to read the json resultset and look at each object in the "items" collection, specifically dstLinks
 
you could also use some more filters to reduce the data that is returned - e.g.
includeSrcLinks=false
associations=com.infa.ldm.entity.collaboration.CollaborationCommentAssociation

=========
FROM
https://network.informatica.com/thread/88028
Get all the asset types from EDC using API in use and their count
url: https://infaserveril26zi7vnj4uo.westeurope.cloudapp.azure.com:9085/access/2/catalog/data/search?q=*&facet=true&defaultFacets=false&facetId=core.classType&highlight=false&offset=0&pageSize=1
output: examples/allclasses_in_use_and counts_output.json

To get all classes from all models:
url: https://infaserveril26zi7vnj4uo.westeurope.cloudapp.azure.com:9085/access/2/catalog/models/classes
output: examples/allclasses_output.json


=========
FROM 
https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/duplicating-a-repository

duplicated azure-sdk-for-python


========
Import csv for custom etl
curl -X POST "https://infaserveril26zi7vnj4uo.westeurope.cloudapp.azure.com:9085/access/2/catalog/jobs/objectImports" -H "accept: application/json" -H "Content-Type: multipart/form-data" -F "file=@objects_Agg.csv;type=text/csv" -F "packages=org.demo.custom.etl001.Mapping,org.demo.custom.etl001.Transformation,org.demo.custom.etl001.Group,org.demo.custom.etl001.Field" -F "emailId=beekersjac@gmail.com"

Tryout for default objects:
curl -X POST "https://infaserveril26zi7vnj4uo.westeurope.cloudapp.azure.com:9085/access/2/catalog/jobs/objectImports" -H "accept: application/json" -H "Content-Type: multipart/form-data" -F "file=@optimus_objects.csv;type=text/csv" -F "packages=com.infa.ldm.etlcore.Mapping,com.infa.ldm.etlcore.Transformation,com.infa.ldm.etlcore.Group,com.infa.ldm.etlcore.Field" -F "emailId=beekersjac@gmail.com"

response body:
{
  "jobId": "objectImports_94af98f8082c893bd944ce014d2e23e5",
  "createdTime": 0,
  "startTime": 0,
  "logHref": "/2/catalog/jobs/objectImports_94af98f8082c893bd944ce014d2e23e5/logs/zip",
  "configHref": "/2/catalog/jobs/objectImports/objectImports_94af98f8082c893bd944ce014d2e23e5",
  "status": "SUBMITTED"
}

another tryout
com.infa.ldm.etl.pc.Mapping,com.infa.ldm.etl.pc.CustomTransformation,com.infa.ldm.etl.pc.Group,com.infa.ldm.etl.pc.Field


=====
create a resource, offline model: PowerCenter
curl -X POST "https://infaserveril26zi7vnj4uo.westeurope.cloudapp.azure.com:9085/access/1/catalog/resources" -H "accept: application/json" -H "from: Jac" -H "securityDomain: Native" -H "isAdministrator: true" -H "Content-Type: application/json" -d "{ \"createdBy\": \"Jac.\", \"createdTime\": 0, \"modifiedBy\": \"Jac.\", \"modifiedTime\": 0, \"resourceIdentifier\": { \"description\": \"Dummy for EDC\", \"resourceName\": \"rs_demo_offline\", \"resourceTypeId\": \"PowerCenter\", \"resourceTypeName\": \"PowerCenter\", \"resourceTypeVersion\": \"\" }, \"scannerConfigurations\": [ { \"configOptions\": [ { \"optionId\": \"ScannerExecutionMode\", \"optionValues\": [ \"offline\" ] }, { \"optionId\": \"Informatica Security Domain\", \"optionValues\": [ \"Native\" ] }, { \"optionId\": \"PowerCenter Code Page\", \"optionValues\": [ \"ISO 8859-1 Western European\" ] }, { \"optionId\": \"Gateway Host Name or Address\", \"optionValues\": [ \"localhost\" ] }, { \"optionId\": \"Gateway Port Number\", \"optionValues\": [ \"12340\" ] }, { \"optionId\": \"Repository Name\", \"optionValues\": [ \"RepositoryName\" ] }, { \"optionId\": \"Repository User Name\", \"optionValues\": [ \"RepositoryUserName\" ] }, { \"optionId\": \"Repository User Password\", \"optionValues\": [ \"RepositoryUserPassword\" ] }, { \"optionId\": \"PowerCenter Version\", \"optionValues\": [ \"10.4.1\" ] }, { \"optionId\": \"AutoAssignConnections\", \"optionValues\": [ \"false\" ] }, { \"optionId\": \"DetailedLineage\", \"optionValues\": [ \"true\" ] }, { \"optionId\": \"Parameter File\", \"optionValues\": [ \"ParameterFile.zip\" ] }, { \"optionId\": \"Repository subset\", \"optionValues\": [ \"'FolderName1'/*/*\", \"'FolderName2'/*/*\", \"'FolderName3'/*/*\" ] }, { \"optionId\": \"Memory\", \"optionValues\": [ \"Low\" ] } ], \"enabled\": true, \"scanner\": { \"providerTypeId\": \"CORE\", \"providerTypeName\": \"Metadata Load\", \"scannerId\": \"PowerCenterScanner\" } } ]}"


